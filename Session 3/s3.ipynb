{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9206b3e1",
   "metadata": {},
   "source": [
    "# Session 3 – Unsupervised Learning: Dimensionality Reduction & Clustering\n",
    "\n",
    "By the end of this notebook, we will:\n",
    "- Understand unsupervised learning and how it differs from supervised tasks\n",
    "- Apply Principal Component Analysis (PCA) for dimensionality reduction and visualization\n",
    "- Try out other methods such us t-SNE and UMAP\n",
    "- Perform K-Means clustering to find patterns in unlabeled data\n",
    "- Interpret cluster assignments and compare them with true labels (if available)\n",
    "- Explore hierarchical clustering\n",
    "\n",
    "We’ll reuse the **Breast Cancer dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf80bf6",
   "metadata": {},
   "source": [
    "### Load the Breast Cancer dataset\n",
    "\n",
    "We’ll ignore labels during clustering — but later compare clusters to the true diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbf04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y = cancer.target  # ground truth, used only for evaluation\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746b9b3",
   "metadata": {},
   "source": [
    "## Part 1: Dimensionality Reduction\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA transforms correlated features into a smaller set of uncorrelated components capturing the most variance.\n",
    "\n",
    "In order to perform PCA we must scale the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b73c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75364624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e574c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b153d9c1",
   "metadata": {},
   "source": [
    "### Variance explained for each of the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f8577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be32b41",
   "metadata": {},
   "source": [
    "### Visualize the first two components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c899d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a036406",
   "metadata": {},
   "source": [
    "### Exercise 1: Visualize the explained variance as a cumulative function\n",
    "\n",
    "1. Plot as a line plot the Cumulative Explained Variance vs the number of PCA Components.\n",
    "2. How many component explain 90% of the variance?\n",
    "3. How many components would you select? \n",
    "\n",
    "*Hints: Use np.cumsum()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4347b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077a483",
   "metadata": {},
   "source": [
    "### Exercise 2: Visualize the first two components but colored by the target value.\n",
    "\n",
    "- Compare the range of values of the x-axis and the y-axis. Does it make sense?\n",
    "- Is PCA able to separate between the two classes (malignant or healthy)?\n",
    "- Compute the PCA again but setting the parameter `n_components=2`. Do you see any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94c454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19f5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca48574",
   "metadata": {},
   "source": [
    "### Exercise 3: Plot the PCA component loadings.\n",
    "\n",
    "Plotting the loadings helps us see which original features contribute the most. \n",
    "\n",
    "*Hints:* Use pca.compontents_ to get the weights for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c81cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fa566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913871fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a593aa8",
   "metadata": {},
   "source": [
    "### Exercise 4: Compare PCA with t-SNE \n",
    "\n",
    "In this exercise we want to compare:\n",
    "- **PCA** (linear)\n",
    "- **t-SNE** (non-linear, good for local structure)\n",
    "\n",
    "Tasks:\n",
    "1. Compute 2D embeddings for each method using the standardized feature matrix (`X_scaled`).\n",
    "2. Plot the 2D scatter for each method side-by-side, coloring by the true labels (`y`) to visually compare separation.\n",
    "3. Discuss differences: Which method gives the most visually separated classes? Which scores are higher? How do runtime and stability compare?\n",
    "4. (Optional) Try different parameters for TSNE and compare.\n",
    "\n",
    "*Hints:*\n",
    "- Use `sklearn.manifold.TSNE` for t-SNE.\n",
    "- t-SNE has parameters `perplexity` and `n_iter`. Try `perplexity=30`, `n_iter=1000`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5defa",
   "metadata": {},
   "source": [
    "**1. Compute 2D embeddings for each method using the standardized feature matrix (`X_scaled`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9f927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f880be53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e78bcc17",
   "metadata": {},
   "source": [
    "**2. Plot the 2D scatter for each method side-by-side, coloring by the true labels (`y`) to visually compare separation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b2084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b11c399a",
   "metadata": {},
   "source": [
    "**4. (Optional) Try different parameters for TSNE and compare.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363625e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c5d3621",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 5: Install and compare with UMAP\n",
    "\n",
    "If you want, you can install umap and run the comparison with the other methods. To install it, try `pip install umap-learn`.\n",
    "\n",
    "Then:\n",
    "\n",
    "`reducer = umap.UMAP(n_components=2, random_state=42)`\n",
    "\n",
    "`X_umap = reducer.fit_transform(X_scaled)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4773181",
   "metadata": {},
   "source": [
    "## Part 2: Clustering\n",
    "\n",
    "### K-Means Clustering\n",
    "\n",
    "Now we’ll let an unsupervised algorithm find groups in the data projected by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21618c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# X_pca needs to have been computed in the previous section first\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "y_kmeans = kmeans.fit_predict(X_pca)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_kmeans, palette='viridis', alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], \n",
    "            s=200, c='red', marker='X', label='Centers')\n",
    "plt.title(\"K-Means Clusters on PCA Projection\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1a889",
   "metadata": {},
   "source": [
    "### Computing some clustering metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "ari = adjusted_rand_score(y, y_kmeans)\n",
    "sil = silhouette_score(X_pca, y_kmeans)\n",
    "\n",
    "print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
    "print(f\"Silhouette Score: {sil:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ac045",
   "metadata": {},
   "source": [
    "### Exercise 1: Try different k values for the cluster.\n",
    "\n",
    "- Loop over k=2...10. Store inertia_ and plot.\n",
    "- What is the appropriate number of clusters?\n",
    "- Now try different seeds. How is the cluster stability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54c83b",
   "metadata": {},
   "source": [
    "**Loop over k=2...10. Store inertia_ and plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ededa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7d0d7e",
   "metadata": {},
   "source": [
    "**Now try different seeds. How is the cluster stability?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10afe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319c899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48c90465",
   "metadata": {},
   "source": [
    "### Exercise 2: Try Hierarchical clustering instead.\n",
    "\n",
    "- Use `AgglomerativeClustering` with 2 clusters and visualzie the dendrogram.\n",
    "\n",
    "*Hint:* from sklearn.cluster import AgglomerativeClustering, and from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affd0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e78d6d",
   "metadata": {},
   "source": [
    "### Exercise 3: Exploring Distance Metrics\n",
    "\n",
    "Most clustering and dimensionality reduction algorithms rely on a *distance* or *similarity* measure to define how close two samples are.\n",
    "\n",
    "In this exercise, we’ll explore three common metrics:\n",
    "\n",
    "| Metric | Notes |\n",
    "|----------|-------|\n",
    "| **Euclidean** | Standard straight-line distance (default in K-Means) |\n",
    "| **Manhattan** | Sum of absolute differences (“city block” distance) |\n",
    "| **Cosine similarity** | Measures *angle* between vectors (scale-invariant) |\n",
    "\n",
    "You will:\n",
    "1. Compute pairwise distances between samples.  \n",
    "2. Visualize how the metric affects clustering using K-Means.  \n",
    "3. Use t-SNE with custom metrics to show how neighborhood relationships change when using these three different metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efca7c",
   "metadata": {},
   "source": [
    "**1. Compute pairwise distances between samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62906e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98433454",
   "metadata": {},
   "source": [
    "**2. Visualize how the metric affects clustering using K-Means.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721a91e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ce1f4a",
   "metadata": {},
   "source": [
    "**3. Use t-SNE with custom metrics to show how neighborhood relationships change when using these three different metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd090a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66899e3f",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 4: Explore density-based clustering and compare with the approaches we learned today "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b75411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
